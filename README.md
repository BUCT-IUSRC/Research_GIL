# Research_GIL-A-Generative-Incremental-Learning-Paradigm-for-Open-World-Object-Detection
## <font style="color:rgb(31, 35, 40);">Framework Overview</font>![](pipeline.png)
![微信截图_20250616214432](https://github.com/user-attachments/assets/3f3e2cfc-9882-4861-aba9-85bb40b78f8b)

The overall architecture of our proposed GIL.

Open-world object detection (OWOD) not only detects known objects but also enables recognition of unseen categories in dynamic and constantly changing environments. The previous OWOD methods tend to exploit pseudo-labels to generate supervision or learn a probability model for dealing with unseen categories, while the precise unknown object detection remains challenging. To address this issue, we introduce a novel OWOD framework based on a generative incremental learning (GIL) paradigm, which achieves open-world object detection by incorporating attribute category prior knowledge generated by large language models into visual detection models. First, GIL integrates category-independent semantic object attributes to capture the implicit dependencies between images and diverse attributes. This is achieved by orthogonally fusing attribute textual information with two-dimensional visual features to effectively measure cross-modal correspondence.
Then, we introduce a selection strategy for the broad features generated by the LLM, enabling the supervised learning of known category information to facilitate the unsupervised learning of unknown categories to enhance the generalization ability of the object detection head. Furthermore, stage-wise adaptation and sample replay mechanisms are considered during training process to prevent catastrophic forgetting of old categories when learning new ones. Experimental evaluations on two benchmark datasets, M-OWODB and S-OWODB, demonstrate the effectiveness of our proposed method.

## **<font style="color:rgb(38, 38, 38);">Install</font>**
### Environment
<font style="color:rgb(31, 35, 40);">We have trained and tested our models on Ubuntu 16.04, CUDA 11.1/11.3, GCC 5.4.0, Python 3.10.4</font>

```plain
conda create --name prob python==3.10.4
conda activate prob
pip install -r requirements.txt
pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113
```
### Backbone
Download the self-supervised backbone from </font>[<font style="color:rgb(9, 105, 218);">here</font>](https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth)<font style="color:rgb(31, 35, 40);"> and add in models folder.

### Compiling CUDA operators


```plain
cd ./models/ops
sh ./make.sh
# unit test (should see all checking is True)
python test.py
```

### Dataset Preparation
<font style="color:rgb(31, 35, 40);">The file structure:</font>
```plain
PROB/
└── data/
    └── OWOD/
        ├── JPEGImages
        ├── Annotations
        └── ImageSets
            ├── OWDETR
            ├── TOWOD
            └── VOC2007
```
The splits are present inside data/OWOD/ImageSets/ folder.
1. Download the COCO Images and Annotations from </font><font style="color:rgb(31, 35, 40);"> </font>[<font style="color:rgb(9, 105, 218);">coco dataset</font>](https://cocodataset.org/#download)<font style="color:rgb(31, 35, 40);"> into the data/ directory.
2. Unzip train2017 and val2017 folder. The current directory structure should look like:
```
PROB/
└── data/
    └── coco/
        ├── annotations/
        ├── train2017/
        └── val2017/
```
3. Move all images from train2017/ and val2017/ to JPEGImages folder.
4. Use the code coco2voc.py for converting json annotations to xml files.
5. Download the PASCAL VOC 2007 & 2012 Images and Annotations from </font><font style="color:rgb(31, 35, 40);"> </font>[<font style="color:rgb(9, 105, 218);">pascal dataset</font>](http://host.robots.ox.ac.uk/pascal/VOC/)<font style="color:rgb(31, 35, 40);"> into the data/ directory.
6. untar the trainval 2007 and 2012 and test 2007 folders.
7. Move all the images to JPEGImages folder and annotations to Annotations folder.

Currently, we follow the VOC format for data loading and evaluation

### <font style="color:rgb(31, 35, 40);">Training</font>
To train PROB on a single node with 4 GPUS, run
```plain
bash ./run.sh
```
**note: you may need to give permissions to the .sh files under the 'configs' and 'tools' directories by running chmod +x *.sh in each directory.
By editing the run.sh file, you can decide to run each one of the configurations defined in \configs:
1. EVAL_M_OWOD_BENCHMARK.sh - evaluation of tasks 1-4 on the MOWOD Benchmark.
2. EVAL_S_OWOD_BENCHMARK.sh - evaluation of tasks 1-4 on the SOWOD Benchmark.
3. M_OWOD_BENCHMARK.sh - training for tasks 1-4 on the MOWOD Benchmark.
4. M_OWOD_BENCHMARK_RANDOM_IL.sh - training for tasks 1-4 on the MOWOD Benchmark with random exemplar selection.
5. S_OWOD_BENCHMARK.sh - training for tasks 1-4 on the SOWOD Benchmark.

### <font style="color:rgb(31, 35, 40);">Evaluation</font>
Run the run_eval.sh file to utilize multiple GPUs.

**note: you may need to give permissions to the .sh files under the 'configs' and 'tools' directories by running chmod +x *.sh in each directory.
```plain
PROB/
└── exps/
    ├── MOWODB/
    |   └── PROB/ (t1.ph - t4.ph)
    └── SOWODB/
        └── PROB/ (t1.ph - t4.ph)
```




## Acknowledgement
<font style="color:rgb(31, 35, 40);">Many thanks to these excellent open-source projects:</font>

+ [<font style="color:rgb(9, 105, 218);">OW-DETR</font>](https://github.com/akshitac8/OW-DETR)
+ [Deformable DETR](https://github.com/fundamentalvision/Deformable-DETR)
+ [Detreg](https://github.com/amirbar/DETReg)
+ [OWOD](https://github.com/amirbar/DETReg)
+ [FOMO](https://github.com/orrzohar/FOMO)
